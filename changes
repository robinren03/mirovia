src/cuda/common/main.cpp
- added -outputFile option to write benchmark results to a file instead of stdout
- added -inputFile option to read data from a file
- changed the output from ResultDatabase.DumpDetailed() to ResultDatabase.DumpSummary()

data/
- the idea of having a datagen for certain benchmarks is to allow the user to customize the benchmark to their needs, rather than having to use a limit amount of preset data sizes with a fixed configuration
- if the problem size that the user wants to use is very large, the datagen saves time by writing a file that can be read repeatedly, instead of the benchmark having to generate the data over and over again
- replacing Rodinia benchmark datagens with python version for consistency (SHOC benchmark datagens are in python)
- adding datagen to SHOC benchmarks
- user can either
    1. specify a problem size or
    2. specify an input file generated by the datagen

driver.py
- allows the use to run the entire benchmark suite with one command
- driver options:
    - prefix (location of mirovia root)
    - problem size to run benchmarks with
    - device
    - list of benchmarks to run
- can run all or subset of benchmarks
- write benchmark output to results/ folder
- TODO: determine a problem size based on device capabilities
- TODO: calculate & output umbrella result from ResultDB

all benchmarks
- set preset problem sizes based on capabilities of modern gpus
- if using a random number generator, #define SEED 7
- added metrics for kernel time, transfer time, total time

device memory
- when testing device memory capabilities, increased max size to 1GB (from 64MB)
- why its here:
    - tests a low level capability of the device

maxflops
- added half precision tests using <short> (if compute capability supports this)
- why its here:
    - tests a low level capability of the device

sort
- added functionality to read data from a file (TODO: error checking)
- if using preset problem size, use rand() to fill array (instead of i % 1024)
- datagen.py options:
    - size of array
- why its here:
    - basic parallel algorithm that encompasses one of the 9 primitives
    - dwarf: sorting algorithm

gemm
- added functionality to read data from a file
- moved data size option (in kb) to datagen.py
- datagen.py options:
    - size of matrix (in kb)
- why its here:
    - basic parallel algorithm
    - tests performance of cuda feature: hyperq

pathfinder
- added ability to do multiple passes
- why its here:
    - basic parallel algorithm that encompasses one of the 9 primitives
    - dwarf: dynamic programming

bfs
- added ability to do multiple passes
- initGraph() initializes graph once, runs bfs multiple times
- can read graph from a file or initialize graph of preset problem size
- implemented bfs using unified memory (host & device "share" memory)
- datagen.py options:
    - size of graph (# of nodes)
- why its here:
    - basic parallel algorithm that encompasses one of the 9 primitives
    - dwarf: graph traversal
    - tests performance of cuda feature: unified memory

nw
- added ability to do multiple passes
- why its here:
    - real application kernel that covers a domain currently dominated by GPUs
    - domain: bioinformatics

particle filter
- Didn't include the use_fast_math flag (from testing it only adds ~0.3% speed improvement)
- Kept float and naive as separate binaries, (difficult to merge them into single with a flag)
- why its here:
    - real application kernel that covers a domain currently dominated by GPUs
    - domain: medical imaging

lavamd
- TODO

Rodinia is heavily fragmented with each benchmark living in it's own bubble (specifies its own arch value, own location for /usr/local/cuda)
we're bringing the uniformity of shoc (get all config info from universal source) over to rodinia

We did our best to choose benchmarks that not only cover all 9 primitives, but also cover a wide variety of application domains where GPUs are dominating.
